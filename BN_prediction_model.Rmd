---
title: An R Markdown document converted from "/Users/emmafoessing/Documents/Master/MA/Code/Master-Thesis/BN_prediction_model.ipynb"
output: html_document
---

# Baysian Networks prediction function

```{r}
# directory
directory <- "/Users/emmafoessing/Documents/Master/MA/Code/Master-Thesis"
```

### Packages

```{r}
list_of_packages <- c ("synthpop", "insight", "party", "haven", "dplyr", "rpart", "rpart.plot", "randomForest", "pROC", "caret", "pracma", "here", "Hmisc", "purrr", "randomForest", "ranger", "bnlearn", "arulesCBA", "network", "igraph")

install_if_missing <- function(p){
  if(!requireNamespace(p, quietly = TRUE)){
    install.packages(p)
  }
  library(p, character.only=TRUE)
}


lapply(list_of_packages, install_if_missing)
```

### Data

```{r}
load(paste0(directory, "/cpspop.RData"))
adult <- read.csv(paste0(directory, "/adult_preprocessed.csv"))
# delete NAs
adult[adult == "?"] <- NA
adult <- na.omit(adult)

adult$workclass <- as.factor(adult$workclass)
adult$education <- as.factor(adult$education)
adult$marital_status <- as.factor(adult$marital_status)
adult$relationship <- as.factor(adult$relationship)
adult$race <- as.factor(adult$race)
adult$sex <- as.factor(adult$sex)
adult$native_country <- as.factor(adult$native_country)
adult$income <- as.factor(adult$income)
adult$occupation <- as.factor(adult$occupation)

adult_with_cont <- adult
cps_with_cont <- cpspop
```

### Helper functions

```{r}
evaluation_metrics_factor <- function(predictions, test_set) {
    # Ensure test_set is a data frame
    test_set <- as.data.frame(test_set)
    
    # Ensure both predictions and test_set$income are factors with the same levels
    predictions <- as.factor(predictions)
    reference <- as.factor(test_set$income)
    
    # Ensure levels match between predictions and reference
    levels(predictions) <- levels(reference)
    
    # Confusion matrix for the prediction on original data
    cm <- confusionMatrix(predictions, reference, mode = "everything")

    # Saving evaluation metrics
    accuracy <- cm$overall['Accuracy']
    
    if (length(levels(reference)) == 2) {
        # Binary classification
        f1 <- cm$byClass['F1']
        sens <- cm$byClass['Sensitivity']
        spec <- cm$byClass['Specificity']
    } else {
        # Multi-class classification: calculate metrics for each class and take the mean
        f1 <- mean(cm$byClass[,'F1'], na.rm = TRUE)
        sens <- mean(cm$byClass[,'Sensitivity'], na.rm = TRUE)
        spec <- mean(cm$byClass[,'Specificity'], na.rm = TRUE)
    }

    # Create the dataframe
    metrics_df <- data.frame(
        Accuracy = accuracy, 
        F1 = f1, 
        Sensitivity = sens, 
        Specificity = spec
    )
    
    return(metrics_df)
}
```

Ich diskretisiere alle meine continuous cars (in Intervalle einteilen) --> das geht mit der discretize function <br>
Ich will ca. 5 Kategorien maximal pro Variable haben. Wenn eine der Auspr채gungen in der Varibale mindestens 1/5 aller Auspr채gungnen aus macht, dann soll diese eine eigene Kategorie werden und die restlichen als Interval kategorisiert werden. Wenn es mehrere Werte gibt, die mindestens 1/5 aller Auspr채gungen ausmachen, dann sollen auch diese alle jeweils eine eigene Kategorie werden und der Rest kann in Intervalle eingeteilt werden. Am Ende soll die Variable des Datensatzes 체berschrieben werden mit der kategorialen Variable und der Datensatz nur aus 'factor' Variablen bestehen.
2/5 sind =0 --> 3 weitere Kategorien mit Intervallen
3/5 sind =0 --> 2 weitere Kategorien mit Intervallen

```{r}
discretize_df = function(df, breaks = 5) {
  for (var in colnames(df)) {
    # Check if the variable is not a factor
    if (!is.factor(df[[var]])) {

      # Count the frequency of each unique value
      freq_table <- table(df[[var]])

      # Calculate the proportion of zeros, ensuring NA is handled
      zero_proportion <- ifelse(!is.na(freq_table[as.character(0)]), 
                                freq_table[as.character(0)] / sum(freq_table), 
                                0)

      # Determine the number of breaks based on zero proportion
      if (zero_proportion > 4/5) {
        new_breaks = 1
      } else if (zero_proportion > 1/4) {
        new_breaks = breaks - 2
      } else if (zero_proportion > 1/5) {
        new_breaks = breaks - 1
      } else {
        new_breaks = breaks
      }
      
      # Separate zeros and non-zeros
      zero_portion = (df[[var]] == 0)
      non_zero_values = df[[var]][!zero_portion]

      # Discretize non-zero values
      if (length(non_zero_values) > 0) {
        # Calculate breaks for non-zero values
        range_values = range(non_zero_values, na.rm = TRUE)
        breaks_values = seq(range_values[1], range_values[2], length.out = new_breaks + 1)
        
        # Ensure correct number of labels are created
        labels = sapply(1:(length(breaks_values)-1), function(i) 
                        paste("(", breaks_values[i], "-", breaks_values[i+1], "]", sep=""))

        # Use cut to apply these breaks and labels
        discretized_non_zeros = cut(non_zero_values, breaks = breaks_values, labels = labels, include.lowest = TRUE)
        # Combine zero and discretized non-zeros into the original dataframe
        df[[var]] <- factor(ifelse(zero_portion, "0", as.character(discretized_non_zeros)))
      } else {
        # If all values are zero or the number of breaks is zero or negative
        df[[var]] <- factor("0")
      }
    }
  }
  return(df)
}
```

#### Look at the levels created

```{r}
print_levels <- function(data) {
  factor_vars <- sapply(data, is.factor)
  for (var in names(data)[factor_vars]) {
    cat("Levels of", var, ":\n")
    print(levels(data[[var]]))
    cat("\n")
  }
}
```

```{r}
# cps
discretized_data <- discretize_df(cpspop)
print_levels(discretized_data)

# adult
discretized_data <- discretize_df(adult)
print_levels(discretized_data)
```

```{r}
cpdag_to_dag <- function(cpdag) {
  # Convert bnlearn object to adjacency matrix
  adj_matrix <- amat(cpdag)
  
  # Convert adjacency matrix to igraph object
  ig <- graph_from_adjacency_matrix(adj_matrix, mode = "directed")
  
  # Check if it's already a DAG
  if (igraph::is_dag(ig)) {
    return(cpdag)
  }
  
  # Convert CPDAG to DAG by randomly orienting undirected edges
  directed_arcs <- directed.arcs(cpdag)
  undirected_arcs <- undirected.arcs(cpdag)
  
  while (nrow(undirected_arcs) > 0) {
    arc <- undirected_arcs[1, , drop = FALSE]
    cpdag <- set.arc(cpdag, from = arc[1, 1], to = arc[1, 2])
    undirected_arcs <- undirected.arcs(cpdag)
  }
  return(cpdag)
}
```

```{r}
train_bn <- function(data, algorithm, score = NULL) {
  if (algorithm == "hc") {
    bn <- bnlearn::hc(data, score = score)
  } else if (algorithm == "tabu") {
    bn <- bnlearn::tabu(data, score = score)
  } else if (algorithm == "gs") {
    bn <- bnlearn::gs(data)
    bn <- bnlearn::cpdag(bn) # Convert to a completed partially directed acyclic graph
    bn <- cpdag_to_dag(bn) # Convert CPDAG to DAG
  } else if (algorithm == "iamb") {
    bn <- bnlearn::iamb(data)
    bn <- bnlearn::cpdag(bn) # Convert to a completed partially directed acyclic graph
    bn <- cpdag_to_dag(bn) # Convert CPDAG to DAG
  } else {
    stop("Unsupported algorithm")
  }

  cat(algorithm, "\n")

  bn.fit(bn, data)
}
```

```{r}
# Define a function to evaluate the model using cross-validation
evaluate_bn <- function(data, bn_fitted, target_var) {
  predictions <- predict(bn_fitted, data = data, node = target_var)
  actual <- data[[target_var]]
  accuracy <- mean(predictions == actual)
  return(accuracy)
}
```

# Just BN prediction on original data

```{r}
# Define discretize_df function
discretize_df <- function(df, breaks = 5) {
  for (var in colnames(df)) {
    if (!is.factor(df[[var]])) {
      freq_table <- table(df[[var]])
      zero_proportion <- ifelse(!is.na(freq_table[as.character(0)]), 
                                freq_table[as.character(0)] / sum(freq_table), 
                                0)
      if (zero_proportion > 4/5) {
        new_breaks <- 1
      } else if (zero_proportion > 1/4) {
        new_breaks <- breaks - 2
      } else if (zero_proportion > 1/5) {
        new_breaks <- breaks - 1
      } else {
        new_breaks <- breaks
      }
      zero_portion <- (df[[var]] == 0)
      non_zero_values <- df[[var]][!zero_portion]
      if (length(non_zero_values) > 0) {
        range_values <- range(non_zero_values, na.rm = TRUE)
        breaks_values <- seq(range_values[1], range_values[2], length.out = new_breaks + 1)
        labels <- sapply(1:(length(breaks_values) - 1), function(i) 
                         paste("(", breaks_values[i], "-", breaks_values[i + 1], "]", sep = ""))
        discretized_non_zeros <- cut(non_zero_values, breaks = breaks_values, labels = labels, include.lowest = TRUE)
        df[[var]] <- factor(ifelse(zero_portion, "0", as.character(discretized_non_zeros)))
      } else {
        df[[var]] <- factor("0")
      }
    }
  }
  return(df)
}

# Define cpdag_to_dag function
cpdag_to_dag <- function(cpdag) {
  adj_matrix <- amat(cpdag)
  ig <- graph_from_adjacency_matrix(adj_matrix, mode = "directed")
  if (igraph::is_dag(ig)) {
    return(cpdag)
  }
  directed_arcs <- directed.arcs(cpdag)
  undirected_arcs <- undirected.arcs(cpdag)
  while (nrow(undirected_arcs) > 0) {
    arc <- undirected_arcs[1, , drop = FALSE]
    cpdag <- set.arc(cpdag, from = arc[1, 1], to = arc[1, 2])
    undirected_arcs <- undirected.arcs(cpdag)
  }
  return(cpdag)
}

# Define train_bn function
train_bn <- function(data, algorithm, score = NULL) {
  if (any(is.na(data))) {
    stop("The data contains missing values.")
  }
  
  if (algorithm %in% c("hc", "tabu") && !is.null(score)) {
    bn <- bnlearn::hc(data, score = score)
  } else if (algorithm == "tabu" && !is.null(score)) {
    bn <- bnlearn::tabu(data, score = score)
  } else if (algorithm == "gs") {
    bn <- bnlearn::gs(data)
    bn <- bnlearn::cpdag(bn)
    bn <- cpdag_to_dag(bn)
  } else if (algorithm == "iamb") {
    bn <- bnlearn::iamb(data)
    bn <- bnlearn::cpdag(bn)
    bn <- cpdag_to_dag(bn)
  } else {
    stop("Unsupported algorithm or missing score for algorithm.")
  }
  
  bn.fit(bn, data)
}

# Define a function to evaluate the Bayesian network model
evaluate_bn <- function(testData, bn_fitted, target_var) {
  predictions <- predict(bn_fitted, data = testData, node = target_var)
  mean(predictions == testData[[target_var]])
}

custom_model <- list(
  type = c("Classification", "Regression"),
  library = "bnlearn",
  loop = NULL,
  parameters = data.frame(parameter = c("algorithm", "score"),
                          class = c("character", "character"),
                          label = c("Algorithm", "Score")),
  grid = function(x, y, len = NULL, search = "grid") {
    algorithms <- c("hc", "tabu", "gs", "iamb")
    scores <- c("aic", "bic")
    expand.grid(algorithm = algorithms, score = scores)
  },
  fit = function(x, y, wts, param, lev, last, classProbs, ...) {
    data <- as.data.frame(x)
    data$income <- y
    
    print("Fitting model with parameters:")
    print(param)
    
    if (any(is.na(data))) {
      stop("The data contains missing values.")
    }
    
    # Additional debug info
    if (!param$score %in% c("aic", "bic")) {
      stop("Invalid score parameter: ", param$score)
    }
    
    train_bn(data, param$algorithm, param$score)
  },
  predict = function(modelFit, newdata, submodels = NULL) {
    if (any(is.na(newdata))) {
      stop("The new data contains missing values.")
    }
    predict(modelFit, newdata)
  },
  prob = function(modelFit, newdata, submodels = NULL) {
    if (any(is.na(newdata))) {
      stop("The new data contains missing values.")
    }
    predict(modelFit, newdata, type = "prob")
  },
  predictors = function(x, ...) {
    names(x$bn)
  },
  varImp = NULL,
  levels = function(x) x$lev,
  tags = c("Bayesian Network", "Graphical Models"),
  sort = function(x) x
)
```

```{r}
bn_pred <- function(data, outer_folds, inner_folds){############adjust##############

    # discretize the data
    data <- discretize_df(data)

    if (any(is.na(data))) {
        stop("Data contains NA values after discretization")
    }

    # adjust evaluation metric to fit both numeric and factored targets
    summaryFunctionType <- if (is.numeric(data$income)) defaultSummary else multiClassSummary
    # metric: train() uses per default RSME and Accuracy for numeric and factored targets

    #  set control args
    outer_control <- trainControl(method = "cv", number = outer_folds,
                                  summaryFunction = summaryFunctionType,
                                  verboseIter = FALSE,
                                  allowParallel = TRUE)
        
    inner_control <- trainControl(method = "cv", number = inner_folds, 
                                  summaryFunction = summaryFunctionType,
                                  verboseIter = FALSE,
                                  allowParallel = TRUE)

    # Define the grid for hyperparameter tuning
    algorithms <- c("hc", "tabu", "gs", "iamb") ############adjust##############
    scores <- c("aic", "bic") ############adjust##############

    # Create grid
    tunegrid <- expand.grid(algorithm = algorithms, score = scores) ############adjust##############

    # Initialize variables to store results
    outer_results <- list()

    outer_cv_folds = createFolds(data$income, k = outer_folds)
    
    # Outer loop: Cross-validation for model evaluation
    for (i in seq_along(outer_folds)) {
        
        # Split data into outer folds
        outer_test_index = outer_cv_folds[[i]]
        outer_testData = data[outer_test_index,]
        outer_trainData  = data[-outer_test_index,]
        print("outer data folds")
        print(any(is.na(outer_trainData)))
        print(any(is.na(outer_testData)))
        if (any(is.na(outer_trainData))) {
        print(colSums(is.na(outer_trainData)))}

        print("before train")
        # Hyperparameter tuning using inner CV
        # No need for inner loop because "train" does k-fold CV already
        model <- caret::train(income ~ ., 
                        data = outer_trainData, 
                        method = custom_model, ############adjust##############
                        tuneGrid = tunegrid, 
                        trControl = inner_control)#,
                        #metric = metricType)
            

        # Store the best hyperparameters
        best_hyperparameters <- model$bestTune

        # Train the final model on the outer training set with the best hyperparameters
        final_model <- caret::train(income ~ ., 
                             data = outer_trainData, 
                             method = "rpart",############adjust##############
                             trControl = outer_control, 
                             tuneGrid = best_hyperparameters)

        # Testing the final model on the outer test set
        predictions <- predict(final_model, newdata = outer_testData)
        
        if (is.numeric(data$income)) {
            eval <- evaluation_metrics_cont(predictions, outer_testData)
        } else if (is.factor(data$income)) {
            eval <- evaluation_metrics_factor(predictions, outer_testData)
        } else {
            stop("The predicted target has to be numeric or factor.")
        }

        # Store the evaluation metrics for this outer fold
        outer_results[[i]] <- eval
    }

    # Average the evaluation metrics over the outer folds
    eval_avg_outer_folds <- do.call(rbind, outer_results) %>%
                            summarise(across(everything(), mean, na.rm = TRUE))

    

    # Return the average evaluation metrics
    return(eval_avg_outer_folds)
}
```

```{r}
cps_res <- bn_pred(cps_with_cont, 2, 2)
```

```{r}
adult_res <- bn_pred(adult, 2, 2)
```

```{r}
cps_res
```

### Save the results

```{r}
# Bind results
bn_pred_results <- list(cps_res = cps_res, adult_res = adult_res)
# File pth for output
file <- "/user/emma.foessing01/u11969/results/bn_pred_results.RData" 
dir.create(dirname(output_file), recursive = TRUE, showWarnings = FALSE) # create dir if not there
# Save the results to an RData file 
save(results, file = output_file)
```


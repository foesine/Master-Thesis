{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Set the library path\n",
    "#.libPaths(\"/user/emma.foessing01/u11969/new_R_libs\")\n",
    "Sys.setenv(\"PKG_CXXFLAGS\"=\"-std=c++14\")\n",
    "\n",
    "print(R.version.string)\n",
    "\n",
    "# List of required packages\n",
    "list_of_packages <- c(\n",
    "  \"synthpop\", \"jsonlite\", \"codetools\", \"insight\", \"party\", \"haven\", \"dplyr\", \"rpart\", \"rpart.plot\",\n",
    "  \"randomForest\", \"pROC\", \"caret\", \"pracma\", \"here\", \"Hmisc\", \"purrr\",\n",
    "  \"ranger\", \"bnlearn\", \"arulesCBA\", \"network\", \"igraph\", \"xgboost\",\n",
    "  \"data.table\", \"doParallel\", \"parallel\", \"ExtDist\", \"e1071\"\n",
    ")\n",
    "\n",
    "# Function to load packages and handle errors\n",
    "load_if_installed <- function(p) {\n",
    "  tryCatch({\n",
    "    library(p, character.only = TRUE)\n",
    "  }, error = function(e) {\n",
    "    message(sprintf(\"Package '%s' is not installed.\", p))\n",
    "  })\n",
    "}\n",
    "\n",
    "# Load all required packages\n",
    "lapply(list_of_packages, load_if_installed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "load(file = (paste0(here(), \"/cpspop.RData\")))\n",
    "cpspop <- cpspop[, c(setdiff(names(cpspop), c(\"income\", \"race\", \"marital\", \"educ\")), \"income\", \"race\", \"marital\", \"educ\")] #\n",
    "\n",
    "adult <- read.csv(file = (paste0(here(),\"/adult_preprocessed.csv\")))\n",
    "# delete NAs\n",
    "adult[adult == \"?\"] <- NA\n",
    "adult <- na.omit(adult)\n",
    "\n",
    "adult$workclass <- as.factor(adult$workclass)\n",
    "adult$education <- as.factor(adult$education)\n",
    "adult$marital_status <- as.factor(adult$marital_status)\n",
    "adult$relationship <- as.factor(adult$relationship)\n",
    "adult$race <- as.factor(adult$race)\n",
    "adult$sex <- as.factor(adult$sex)\n",
    "adult$native_country <- as.factor(adult$native_country)\n",
    "adult$income <- as.factor(adult$income)\n",
    "adult$occupation <- as.factor(adult$occupation)\n",
    "\n",
    "adult <- adult[, c(\"age\", \"fnlwgt\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"income\", \"sex\", \"race\", \"relationship\", \"marital_status\", \"workclass\", \"occupation\", \"education\", \"native_country\")]\n",
    "adult[] <- lapply(adult, function(col) {\n",
    "  if (is.integer(col)) {\n",
    "    as.numeric(col)\n",
    "  } else {\n",
    "    col\n",
    "  }\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate evaluation metrics for continuous targets\n",
    "evaluation_metrics_cont <- function(predictions, test_set){\n",
    "    # Residuals\n",
    "    residuals <- predictions - test_set$income\n",
    "    \n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE <- mean(abs(residuals))\n",
    "    \n",
    "    # Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
    "    MSE <- mean(residuals^2)\n",
    "    RMSE <- sqrt(MSE)\n",
    "    \n",
    "    # R-squared: Guarding against zero variance in the target\n",
    "    SS_res <- sum(residuals^2)\n",
    "    SS_tot <- sum((test_set$income - mean(test_set$income))^2)\n",
    "    R_squared <- ifelse(SS_tot == 0, NA, 1 - (SS_res / SS_tot))\n",
    "    \n",
    "    # Mean Absolute Percentage Error (MAPE): Handling division by zero\n",
    "    MAPE <- ifelse(any(test_set$income == 0), NA, mean(abs(residuals / test_set$income)) * 100)\n",
    "    \n",
    "    metrics_df <- data.frame(\n",
    "        MAE = MAE, \n",
    "        MSE = MSE, \n",
    "        RMSE = RMSE, \n",
    "        R_squared = R_squared, \n",
    "        MAPE = MAPE\n",
    "    )\n",
    "    \n",
    "    return(metrics_df)\n",
    "}\n",
    "## Calculate evaluation metrics for factored targets\n",
    "evaluation_metrics_factor <- function(predictions, test_set) {\n",
    "    # Ensure test_set is a data frame\n",
    "    test_set <- as.data.frame(test_set)\n",
    "    \n",
    "    # Ensure both predictions and test_set$income are factors with the same levels\n",
    "    predictions <- as.factor(predictions)\n",
    "    reference <- as.factor(test_set$income)\n",
    "    \n",
    "    # Ensure levels match between predictions and reference\n",
    "    levels(predictions) <- levels(reference)\n",
    "    \n",
    "    # Confusion matrix for the prediction on original data\n",
    "    cm <- caret::confusionMatrix(predictions, reference, mode = \"everything\")\n",
    "\n",
    "    # Saving evaluation metrics\n",
    "    accuracy <- cm$overall['Accuracy']\n",
    "    \n",
    "    if (length(levels(reference)) == 2) {\n",
    "        # Binary classification\n",
    "        f1 <- cm$byClass['F1']\n",
    "        sens <- cm$byClass['Sensitivity']\n",
    "        spec <- cm$byClass['Specificity']\n",
    "    } else {\n",
    "        # Multi-class classification: calculate metrics for each class and take the mean\n",
    "        f1 <- mean(cm$byClass[,'F1'], na.rm = TRUE)\n",
    "        sens <- mean(cm$byClass[,'Sensitivity'], na.rm = TRUE)\n",
    "        spec <- mean(cm$byClass[,'Specificity'], na.rm = TRUE)\n",
    "    }\n",
    "\n",
    "    # Create the dataframe\n",
    "    metrics_df <- data.frame(\n",
    "        Accuracy = accuracy, \n",
    "        F1 = f1, \n",
    "        Sensitivity = sens, \n",
    "        Specificity = spec\n",
    "    )\n",
    "    \n",
    "    return(metrics_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM pred Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "svm_pred <- function(data, outer_folds, cost_steps, inner_folds) {\n",
    "    # Rename the levels of the 'income' variable to valid R variable names\n",
    "    data$income <- as.factor(data$income)\n",
    "    levels(data$income) <- c(\"LessThan50K\", \"GreaterThan50K\")\n",
    "    \n",
    "    # Use twoClassSummary for binary classification\n",
    "    summaryFunctionType <- twoClassSummary\n",
    "\n",
    "    # Set control args\n",
    "    outer_control <- trainControl(\n",
    "        method = \"none\", #no extra cross validation, nested CV manually\n",
    "        summaryFunction = summaryFunctionType,\n",
    "        classProbs = TRUE,\n",
    "        verboseIter = FALSE,\n",
    "        allowParallel = FALSE\n",
    "    )\n",
    "\n",
    "    inner_control <- trainControl(\n",
    "        method = \"cv\",\n",
    "        number = inner_folds,\n",
    "        summaryFunction = summaryFunctionType,\n",
    "        classProbs = TRUE,\n",
    "        verboseIter = FALSE,\n",
    "        allowParallel = FALSE#,\n",
    "        #sampling = \"up\"  # Handle class imbalance\n",
    "    )\n",
    "\n",
    "    # Define the grid for hyperparameter tuning\n",
    "    cost_values <- 10^seq(log10(0.001), log10(100), length.out = cost_steps)\n",
    "    tunegrid <- expand.grid(C = cost_values, sigma = 0.1)\n",
    "\n",
    "    # Create stratified outer CV folds\n",
    "    outer_cv_folds <- createFolds(data$income, k = outer_folds, returnTrain = FALSE, list = TRUE)\n",
    "\n",
    "    # Variable to record failed hyperparameter combinations\n",
    "    failed_combinations <- list()\n",
    "\n",
    "    # Sequential outer loop: Cross-validation for model evaluation\n",
    "    outer_results <- lapply(seq_along(outer_cv_folds), function(i) {\n",
    "        tryCatch({\n",
    "            # Split data into outer folds\n",
    "            outer_test_index <- outer_cv_folds[[i]]\n",
    "            outer_testData <- data[outer_test_index, ]\n",
    "            outer_trainData <- data[-outer_test_index, ]\n",
    "\n",
    "            # Diagnostic messages\n",
    "            cat(\"\\nFold\", i, \":\\n\")\n",
    "            cat(\"Number of observations in training data:\", nrow(outer_trainData), \"\\n\")\n",
    "            cat(\"Number of observations in test data:\", nrow(outer_testData), \"\\n\")\n",
    "\n",
    "            # Exclude the target variable from the constant columns check\n",
    "            predictor_columns <- setdiff(names(outer_trainData), \"income\")\n",
    "            constant_columns <- sapply(outer_trainData[, predictor_columns], function(x) length(unique(x)) == 1)\n",
    "            cat(\"Number of constant predictors (excluding target):\", sum(constant_columns), \"\\n\")\n",
    "\n",
    "            if (any(constant_columns)) {\n",
    "                outer_trainData <- outer_trainData[, c(\"income\", predictor_columns[!constant_columns])]\n",
    "                outer_testData <- outer_testData[, c(\"income\", predictor_columns[!constant_columns])]\n",
    "                warning(\"Constant columns removed before model fitting.\")\n",
    "            }\n",
    "\n",
    "            cat(\"Number of predictors after removing constants:\", length(predictor_columns[!constant_columns]), \"\\n\")\n",
    "\n",
    "            # Ensure 'income' is a factor and levels are valid R variable names\n",
    "            outer_trainData$income <- as.factor(outer_trainData$income)\n",
    "            outer_testData$income <- as.factor(outer_testData$income)\n",
    "            levels(outer_trainData$income) <- make.names(levels(outer_trainData$income))\n",
    "            levels(outer_testData$income) <- levels(outer_trainData$income)  # Ensure consistency\n",
    "\n",
    "            # Print the levels of the target variable\n",
    "            cat(\"Levels of target variable 'income' in training data:\\n\")\n",
    "            print(levels(outer_trainData$income))\n",
    "\n",
    "            # Drop unused factor levels in training data\n",
    "            outer_trainData <- droplevels(outer_trainData)\n",
    "\n",
    "            # Align factor levels in test data to match those in training data\n",
    "            factor_vars <- names(outer_trainData)[sapply(outer_trainData, is.factor)]\n",
    "            factor_vars <- setdiff(factor_vars, \"income\")  # Exclude the target variable\n",
    "            for (var in factor_vars) {\n",
    "                outer_testData[[var]] <- factor(outer_testData[[var]], levels = levels(outer_trainData[[var]]))\n",
    "            }\n",
    "\n",
    "            # Check class distribution\n",
    "            cat(\"Class distribution in training data:\\n\")\n",
    "            print(table(outer_trainData$income))\n",
    "\n",
    "            # Create dummy variables for predictor variables\n",
    "            predictor_vars <- predictor_columns[!constant_columns]\n",
    "            predictor_vars <- setdiff(predictor_vars, \"income\")  # Ensure 'income' is excluded\n",
    "\n",
    "            dummies <- dummyVars(~ ., data = outer_trainData[, predictor_vars], fullRank = TRUE)\n",
    "            outer_trainData_dum <- data.frame(predict(dummies, newdata = outer_trainData[, predictor_vars]))\n",
    "            outer_trainData_dum$income <- outer_trainData$income\n",
    "\n",
    "            outer_testData_dum <- data.frame(predict(dummies, newdata = outer_testData[, predictor_vars]))\n",
    "            outer_testData_dum$income <- outer_testData$income\n",
    "\n",
    "            # Remove zero variance predictors\n",
    "            nzv <- nearZeroVar(outer_trainData_dum)\n",
    "            if(length(nzv) > 0) {\n",
    "                # Record names of variables to be removed\n",
    "                nzv_vars <- names(outer_trainData_dum)[nzv]\n",
    "                cat(\"Removing zero variance predictors:\\n\")\n",
    "                print(nzv_vars)\n",
    "                outer_trainData_dum <- outer_trainData_dum[ , -nzv, drop=FALSE]\n",
    "                outer_testData_dum <- outer_testData_dum[ , -nzv, drop=FALSE]\n",
    "            }\n",
    "\n",
    "            # Ensure that the predictors in train and test data match\n",
    "            if(!identical(names(outer_trainData_dum), names(outer_testData_dum))) {\n",
    "                stop(\"Predictor names in training and test data do not match after preprocessing.\")\n",
    "            }\n",
    "\n",
    "            # Check for variables with empty names after preprocessing\n",
    "            if (any(names(outer_trainData_dum) == \"\")) {\n",
    "                stop(\"There are variables with empty names after preprocessing.\")\n",
    "            }\n",
    "\n",
    "            # Ensure 'income' remains a factor with valid levels\n",
    "            outer_trainData_dum$income <- as.factor(outer_trainData_dum$income)\n",
    "            outer_testData_dum$income <- as.factor(outer_testData_dum$income)\n",
    "            levels(outer_trainData_dum$income) <- make.names(levels(outer_trainData_dum$income))\n",
    "            levels(outer_testData_dum$income) <- levels(outer_trainData_dum$income)\n",
    "\n",
    "            # Hyperparameter tuning using inner CV\n",
    "            model <- caret::train(\n",
    "                income ~ .,\n",
    "                data = outer_trainData_dum,\n",
    "                method = \"svmRadial\",\n",
    "                tuneGrid = tunegrid,\n",
    "                trControl = inner_control#,\n",
    "                #metric = \"ROC\"\n",
    "                #preProcess = c(\"center\", \"scale\")\n",
    "            )\n",
    "\n",
    "            # Store the best hyperparameters\n",
    "            best_hyperparameters <- model$bestTune\n",
    "\n",
    "            # Train the final model on the outer training set with the best hyperparameters\n",
    "            final_model <- caret::train(\n",
    "                income ~ .,\n",
    "                data = outer_trainData_dum,\n",
    "                method = \"svmRadial\",\n",
    "                trControl = outer_control,\n",
    "                tuneGrid = best_hyperparameters#,\n",
    "                #metric = \"ROC\"\n",
    "                #preProcess = c(\"center\", \"scale\")\n",
    "            )\n",
    "\n",
    "            # Testing the final model on the outer test set\n",
    "            predictions <- predict(final_model, newdata = outer_testData_dum)\n",
    "\n",
    "            # Evaluate the model\n",
    "            eval <- evaluation_metrics_factor(predictions, outer_testData_dum)\n",
    "\n",
    "            # Return the evaluation metrics for this outer fold\n",
    "            return(eval)\n",
    "        }, error = function(e) {\n",
    "            # Record the failed hyperparameter combination\n",
    "            failed_combinations[[length(failed_combinations) + 1]] <<- list(\n",
    "                fold = i,\n",
    "                C = tunegrid$C,\n",
    "                sigma = tunegrid$sigma,\n",
    "                error = e$message\n",
    "            )\n",
    "            cat(\"Error in fold \", i, \" for hyperparameters C =\", tunegrid$C, \"and sigma =\", tunegrid$sigma, \"\\n\")\n",
    "            message(\"Error in fold \", i, \": \", e$message)\n",
    "            return(NULL)  # Returning NULL for failed folds\n",
    "        })\n",
    "    })\n",
    "\n",
    "    # Filter out any failed folds (NULL values)\n",
    "    outer_results <- outer_results[!sapply(outer_results, is.null)]\n",
    "\n",
    "    if (length(outer_results) == 0) {\n",
    "        stop(\"No valid results from cross-validation. All folds failed.\")\n",
    "    }\n",
    "\n",
    "    # Convert list of results to a dataframe\n",
    "    outer_results_df <- do.call(rbind, outer_results)\n",
    "\n",
    "    # Average the evaluation metrics over the outer folds\n",
    "    eval_avg_outer_folds <- outer_results_df %>%\n",
    "                            summarise(across(everything(), ~ mean(.x, na.rm = TRUE)))\n",
    "\n",
    "    # Print failed combinations\n",
    "    if (length(failed_combinations) > 0) {\n",
    "        cat(\"Failed hyperparameter combinations:\\n\")\n",
    "        print(failed_combinations)\n",
    "    }\n",
    "\n",
    "    # Return the average evaluation metrics\n",
    "    return(eval_avg_outer_folds)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "s <- 1235\n",
    "syndata <- readRDS(paste0(here(), \"/results/\", \"adult\", \"_svm_\", as.character(s), \".rds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "svm_eval <- svm_pred(syndata, 5, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# if it ran through, save svm eval\n",
    "save(svm_eval, file = paste0(here(), \"/results/adult_svm_eval_\", as.character(s) ,\".RData\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "svm_eval"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "With R kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further info\n",
    "Models usable with train() from caret: <br>\n",
    "https://topepo.github.io/caret/train-models-by-tag.html#Model_Tree <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Set the library path\n",
    "#.libPaths(\"/user/emma.foessing01/u11969/new_R_libs\")\n",
    "Sys.setenv(\"PKG_CXXFLAGS\"=\"-std=c++14\")\n",
    "\n",
    "print(R.version.string)\n",
    "\n",
    "# List of required packages\n",
    "list_of_packages <- c(\n",
    "  \"synthpop\", \"jsonlite\", \"codetools\", \"insight\", \"party\", \"haven\", \"dplyr\", \"rpart\", \"rpart.plot\",\n",
    "  \"randomForest\", \"pROC\", \"caret\", \"pracma\", \"here\", \"Hmisc\", \"purrr\",\n",
    "  \"ranger\", \"bnlearn\", \"arulesCBA\", \"network\", \"igraph\", \"xgboost\",\n",
    "  \"data.table\", \"RSNNS\"\n",
    ")\n",
    "\n",
    "# Function to load packages and handle errors\n",
    "load_if_installed <- function(p) {\n",
    "  tryCatch({\n",
    "    library(p, character.only = TRUE)\n",
    "  }, error = function(e) {\n",
    "    message(sprintf(\"Package '%s' is not installed.\", p))\n",
    "  })\n",
    "}\n",
    "\n",
    "# Load all required packages\n",
    "lapply(list_of_packages, load_if_installed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "load(file = (paste0(here(), \"/cpspop.RData\")))\n",
    "adult <- read.csv(file = paste0(here(),\"/adult_preprocessed.csv\"))\n",
    "# delete NAs\n",
    "adult[adult == \"?\"] <- NA\n",
    "adult <- na.omit(adult)\n",
    "\n",
    "adult$workclass <- as.factor(adult$workclass)\n",
    "adult$education <- as.factor(adult$education)\n",
    "adult$marital_status <- as.factor(adult$marital_status)\n",
    "adult$relationship <- as.factor(adult$relationship)\n",
    "adult$race <- as.factor(adult$race)\n",
    "adult$sex <- as.factor(adult$sex)\n",
    "adult$native_country <- as.factor(adult$native_country)\n",
    "adult$income <- as.factor(adult$income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein Datensatz zu genieren (m = 1) ist ausreichend, da ich keine Varianzanalyse machen werde. Damit die Ergebnisse nicht von einem zufälligen Prozess abhängen ist es sinnvoll über ein paar runs Mittelwerte zu bilden (50–100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate evaluation metrics for continuous targets\n",
    "evaluation_metrics_cont <- function(predictions, test_set){\n",
    "    # Residuals\n",
    "    residuals <- predictions - test_set$income\n",
    "    \n",
    "    # Mean Absolute Error (MAE)\n",
    "    MAE <- mean(abs(residuals))\n",
    "    \n",
    "    # Mean Squared Error (MSE) and Root Mean Squared Error (RMSE)\n",
    "    MSE <- mean(residuals^2)\n",
    "    RMSE <- sqrt(MSE)\n",
    "    \n",
    "    # R-squared: Guarding against zero variance in the target\n",
    "    SS_res <- sum(residuals^2)\n",
    "    SS_tot <- sum((test_set$income - mean(test_set$income))^2)\n",
    "    R_squared <- ifelse(SS_tot == 0, NA, 1 - (SS_res / SS_tot))\n",
    "    \n",
    "    # Mean Absolute Percentage Error (MAPE): Handling division by zero\n",
    "    MAPE <- ifelse(any(test_set$income == 0), NA, mean(abs(residuals / test_set$income)) * 100)\n",
    "    \n",
    "    metrics_df <- data.frame(\n",
    "        MAE = MAE, \n",
    "        MSE = MSE, \n",
    "        RMSE = RMSE, \n",
    "        R_squared = R_squared, \n",
    "        MAPE = MAPE\n",
    "    )\n",
    "    \n",
    "    return(metrics_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "## Calculate evaluation metrics for factored targets\n",
    "evaluation_metrics_factor <- function(predictions, test_set){\n",
    "    # confusion matrix for the prediction on original data\n",
    "    cm <- confusionMatrix(predictions, test_set$income,\n",
    "                mode = \"everything\")\n",
    "\n",
    "    # saving evaluation metrics\n",
    "    accuracy <- cm$overall['Accuracy']\n",
    "    f1 <- cm$byClass['F1']\n",
    "    sens <- cm$byClass['Sensitivity']\n",
    "    spec <- cm$byClass['Specificity']\n",
    "\n",
    "    # Create the dataframe\n",
    "    metrics_df <- data.frame(\n",
    "        Accuracy = accuracy, \n",
    "        F1 = f1, \n",
    "        Sensitivity = sens, \n",
    "        Specificity = spec\n",
    "    )\n",
    "    \n",
    "    return(metrics_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# multi-class summary for prediction on conti target, use MSE\n",
    "multiClassSummary <- function(data, lev = NULL, model = NULL) {\n",
    "    mse <- mean((data$obs - data$pred)^2)\n",
    "    c(MSE = mse)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# just the prediction\n",
    "cart_pred <- function(data, outer_folds, cp_steps, inner_folds){############adjust##############\n",
    "    # adjust evaluation metric to fit both numeric and factored targets\n",
    "    summaryFunctionType <- if (is.numeric(data$income)) defaultSummary else multiClassSummary\n",
    "    # metric: train() uses per default RSME and Accuracy for numeric and factored targets\n",
    "\n",
    "    #  set control args\n",
    "    outer_control <- trainControl(method = \"cv\", number = outer_folds,\n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "        \n",
    "    inner_control <- trainControl(method = \"cv\", number = inner_folds, \n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "\n",
    "    # Define the grid for hyperparameter tuning\n",
    "    complexity <- 10^seq(log10(0.0001), log10(0.01), length.out = cp_steps)############adjust##############\n",
    "\n",
    "    # Create grid\n",
    "    tunegrid <- expand.grid(cp = complexity)############adjust##############\n",
    "\n",
    "    # Initialize variables to store results\n",
    "    outer_results <- list()\n",
    "\n",
    "    outer_cv_folds = createFolds(data$income, k = outer_folds)\n",
    "    \n",
    "    # Outer loop: Cross-validation for model evaluation\n",
    "    for (i in seq_along(outer_folds)) {\n",
    "        \n",
    "        # Split data into outer folds\n",
    "        outer_test_index = outer_cv_folds[[i]]\n",
    "        outer_testData = data[outer_test_index,]\n",
    "        outer_trainData  = data[-outer_test_index,]\n",
    "        \n",
    "        # Hyperparameter tuning using inner CV\n",
    "        # No need for inner loop because \"train\" does k-fold CV already\n",
    "        model <- caret::train(income ~ ., \n",
    "                        data = outer_trainData, \n",
    "                        method = \"rpart\", ############adjust##############\n",
    "                        tuneGrid = tunegrid, \n",
    "                        trControl = inner_control,\n",
    "                        control = rpart.control(maxsurrogate = 0, maxcompete = 1) ############adjust##############\n",
    "                        )#,\n",
    "                        #metric = metricType)\n",
    "            \n",
    "\n",
    "        # Store the best hyperparameters\n",
    "        best_hyperparameters <- model$bestTune\n",
    "\n",
    "        # Train the final model on the outer training set with the best hyperparameters\n",
    "        final_model <- caret::train(income ~ ., \n",
    "                             data = outer_trainData, \n",
    "                             method = \"rpart\",############adjust##############\n",
    "                             trControl = outer_control, \n",
    "                             tuneGrid = best_hyperparameters)\n",
    "\n",
    "        # Testing the final model on the outer test set\n",
    "        predictions <- predict(final_model, newdata = outer_testData)\n",
    "        \n",
    "        if (is.numeric(data$income)) {\n",
    "            eval <- evaluation_metrics_cont(predictions, outer_testData) # postResample is a useful caret function\n",
    "        } else if (is.factor(data$income)) {\n",
    "            eval <- evaluation_metrics_factor(predictions, outer_testData)\n",
    "        } else {\n",
    "            stop(\"The predicted target has to be numeric or factor.\")\n",
    "        }\n",
    "\n",
    "        # Store the evaluation metrics for this outer fold\n",
    "        outer_results[[i]] <- eval\n",
    "    }\n",
    "\n",
    "    # Average the evaluation metrics over the outer folds\n",
    "    eval_avg_outer_folds <- do.call(rbind, outer_results) %>%\n",
    "                            summarise(across(everything(), mean, na.rm = TRUE))\n",
    "\n",
    "    \n",
    "\n",
    "    # Return the average evaluation metrics\n",
    "    return(eval_avg_outer_folds)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "rf_pred <- function(data, outer_folds, mtry_steps, ntree_steps, inner_folds) {\n",
    "    # Adjust evaluation metric to fit both numeric and factored targets\n",
    "    summaryFunctionType <- if (is.numeric(data$income)) defaultSummary else multiClassSummary\n",
    "    # Metric: train() uses per default RSME and Accuracy for numeric and factored targets\n",
    "\n",
    "    # Set control args\n",
    "    outer_control <- trainControl(method = \"cv\", number = outer_folds,\n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "        \n",
    "    inner_control <- trainControl(method = \"cv\", number = inner_folds, \n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "\n",
    "    # Define the parameter grid for tuning\n",
    "    splitrule_value <- if (is.numeric(data$income)) \"variance\" else \"gini\"\n",
    "    \n",
    "    tunegrid <- expand.grid(mtry = seq(2, ncol(data) - 1, length.out = mtry_steps),\n",
    "                            splitrule = splitrule_value,\n",
    "                            min.node.size = 5)\n",
    "    # You can incorporate ntree_steps into the grid if needed\n",
    "\n",
    "    # Initialize variables to store results\n",
    "    outer_cv_folds <- createFolds(data$income, k = outer_folds)\n",
    "\n",
    "    # Initialize variables to store results\n",
    "    outer_results <- list()\n",
    "\n",
    "    outer_cv_folds <- createFolds(data$income, k = outer_folds)\n",
    "    \n",
    "    # Outer loop: Cross-validation for model evaluation\n",
    "    for (i in seq_along(outer_cv_folds)) {\n",
    "        # Split data into outer folds\n",
    "        outer_test_index <- outer_cv_folds[[i]]\n",
    "        outer_testData <- data[outer_test_index, ]\n",
    "        outer_trainData <- data[-outer_test_index, ]\n",
    "        \n",
    "        # Hyperparameter tuning using inner CV\n",
    "        model <- caret::train(income ~ ., \n",
    "                              data = outer_trainData, \n",
    "                              method = \"ranger\",  \n",
    "                              tuneGrid = tunegrid, \n",
    "                              trControl = inner_control)\n",
    "\n",
    "        # Store the best hyperparameters\n",
    "        best_hyperparameters <- model$bestTune\n",
    "\n",
    "        # Train the final model on the outer training set with the best hyperparameters\n",
    "        final_model <- caret::train(income ~ ., \n",
    "                                    data = outer_trainData, \n",
    "                                    method = \"ranger\", \n",
    "                                    trControl = outer_control, \n",
    "                                    tuneGrid = best_hyperparameters)\n",
    "\n",
    "        # Testing the final model on the outer test set\n",
    "        predictions <- predict(final_model, newdata = outer_testData)\n",
    "        \n",
    "        if (is.numeric(data$income)) {\n",
    "            eval <- evaluation_metrics_cont(predictions, outer_testData)\n",
    "        } else if (is.factor(data$income)) {\n",
    "            eval <- evaluation_metrics_factor(predictions, outer_testData)\n",
    "        } else {\n",
    "            stop(\"The predicted target has to be numeric or factor.\")\n",
    "        }\n",
    "\n",
    "        # Store the evaluation metrics for this outer fold\n",
    "        outer_results[[i]] <- eval\n",
    "    }\n",
    "\n",
    "    # Average the evaluation metrics over the outer folds\n",
    "    eval_avg_outer_folds <- do.call(rbind, outer_results) %>%\n",
    "                            summarise(across(everything(), mean, na.rm = TRUE))\n",
    "\n",
    "    # Return the average evaluation metrics\n",
    "    return(eval_avg_outer_folds)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "xgb_pred <- function(data, outer_folds, inner_folds, nrounds_steps, max_depth_steps, eta_steps, gamma_steps, colsample_bytree_steps, min_child_weight_steps, subsample_steps) { ############adjust##############\n",
    "\n",
    "    # Adjust evaluation metric to fit both numeric and factored targets\n",
    "    summaryFunctionType <- if (is.numeric(data$income)) defaultSummary else multiClassSummary\n",
    "    \n",
    "    # Set control args\n",
    "    outer_control <- caret::trainControl(method = \"cv\", number = outer_folds,\n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "        \n",
    "    inner_control <- caret::trainControl(method = \"cv\", number = inner_folds, \n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "\n",
    "    # Define the parameter grid for tuning\n",
    "    tunegrid <- expand.grid(\n",
    "        nrounds = seq(50, 150, length.out = nrounds_steps),\n",
    "        max_depth = round(seq(3, 9, length.out = max_depth_steps)),\n",
    "        eta = seq(0.01, 0.3, length.out = eta_steps),\n",
    "        gamma = seq(0, 0.2, length.out = gamma_steps),\n",
    "        colsample_bytree = seq(0.5, 1, length.out = colsample_bytree_steps),\n",
    "        min_child_weight = seq(1, 10, length.out = min_child_weight_steps),\n",
    "        subsample = seq(0.5, 1, length.out = subsample_steps)\n",
    "    )\n",
    "    # alternatives for smaller grid:\n",
    "        #gamma = 0,  # Default value\n",
    "        #colsample_bytree = 0.8,  # Default value\n",
    "        #min_child_weight = 1,  # Default value\n",
    "        #subsample = 0.8  # Default value\n",
    "\n",
    "\n",
    "    # Initialize variables to store results\n",
    "    outer_results <- list()\n",
    "\n",
    "    outer_cv_folds <- createFolds(data$income, k = outer_folds)\n",
    "    \n",
    "    # Outer loop: Cross-validation for model evaluation\n",
    "    for (i in seq_along(outer_cv_folds)) {\n",
    "        \n",
    "        # Split data into outer folds\n",
    "        outer_test_index <- outer_cv_folds[[i]]\n",
    "        outer_testData <- data[outer_test_index,]\n",
    "        outer_trainData <- data[-outer_test_index,]\n",
    "        \n",
    "        # Convert data for xgboost\n",
    "        train_X <- convert_to_numeric_matrix(outer_trainData[, !colnames(outer_trainData) %in% 'income'])\n",
    "        train_y <- outer_trainData$income\n",
    "        \n",
    "        val_X <- convert_to_numeric_matrix(outer_testData[, !colnames(outer_testData) %in% 'income'])\n",
    "        val_y <- outer_testData$income\n",
    "        \n",
    "        train_dmatrix <- xgb.DMatrix(data = train_X, label = train_y)\n",
    "        val_dmatrix <- xgb.DMatrix(data = val_X, label = val_y)\n",
    "\n",
    "        # Hyperparameter tuning using inner CV\n",
    "        model <- caret::train(x = train_X, \n",
    "                              y = train_y,\n",
    "                              method = \"xgbTree\", ############adjust##############\n",
    "                              tuneGrid = tunegrid, \n",
    "                              trControl = inner_control,\n",
    "                              verbose = FALSE)\n",
    "        \n",
    "        # Store the best hyperparameters\n",
    "        best_hyperparameters <- model$bestTune\n",
    "\n",
    "        # Train the final model on the outer training set with the best hyperparameters\n",
    "        final_model <- caret::train(x = train_X, \n",
    "                                    y = train_y, \n",
    "                                    method = \"xgbTree\", ############adjust##############\n",
    "                                    tuneGrid = best_hyperparameters,\n",
    "                                    trControl = outer_control,\n",
    "                                    verbose = FALSE)\n",
    "\n",
    "        # Testing the final model on the outer test set\n",
    "        predictions <- predict(model, newdata = val_X, iteration_range = c(1, model$bestTune$nrounds))############adjust##############\n",
    "\n",
    "        \n",
    "        if (is.numeric(data$income)) {\n",
    "            eval <- postResample(predictions, val_y)\n",
    "        } else if (is.factor(data$income)) {\n",
    "            eval <- confusionMatrix(predictions, val_y)\n",
    "        } else {\n",
    "            stop(\"The predicted target has to be numeric or factor.\")\n",
    "        }\n",
    "\n",
    "        # Store the evaluation metrics for this outer fold\n",
    "        outer_results[[i]] <- eval\n",
    "    }\n",
    "\n",
    "    # Average the evaluation metrics over the outer folds\n",
    "    eval_avg_outer_fold <- mean(unlist(outer_results)) # Calculate the mean performance over all outer folds\n",
    "\n",
    "    # Return the average evaluation metrics\n",
    "    return(eval_avg_outer_fold)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this does not have an implemented function in the caret::train() function, so the model needs to be created first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "# Define discretize_df function\n",
    "discretize_df <- function(df, breaks = 5) {\n",
    "  for (var in colnames(df)) {\n",
    "    if (!is.factor(df[[var]])) {\n",
    "      freq_table <- table(df[[var]])\n",
    "      zero_proportion <- ifelse(!is.na(freq_table[as.character(0)]), \n",
    "                                freq_table[as.character(0)] / sum(freq_table), \n",
    "                                0)\n",
    "      if (zero_proportion > 4/5) {\n",
    "        new_breaks <- 1\n",
    "      } else if (zero_proportion > 1/4) {\n",
    "        new_breaks <- breaks - 2\n",
    "      } else if (zero_proportion > 1/5) {\n",
    "        new_breaks <- breaks - 1\n",
    "      } else {\n",
    "        new_breaks <- breaks\n",
    "      }\n",
    "      zero_portion <- (df[[var]] == 0)\n",
    "      non_zero_values <- df[[var]][!zero_portion]\n",
    "      if (length(non_zero_values) > 0) {\n",
    "        range_values <- range(non_zero_values, na.rm = TRUE)\n",
    "        breaks_values <- seq(range_values[1], range_values[2], length.out = new_breaks + 1)\n",
    "        labels <- sapply(1:(length(breaks_values) - 1), function(i) \n",
    "                         paste(\"(\", breaks_values[i], \"-\", breaks_values[i + 1], \"]\", sep = \"\"))\n",
    "        discretized_non_zeros <- cut(non_zero_values, breaks = breaks_values, labels = labels, include.lowest = TRUE)\n",
    "        df[[var]] <- factor(ifelse(zero_portion, \"0\", as.character(discretized_non_zeros)))\n",
    "      } else {\n",
    "        df[[var]] <- factor(\"0\")\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  return(df)\n",
    "}\n",
    "\n",
    "# Define cpdag_to_dag function\n",
    "cpdag_to_dag <- function(cpdag) {\n",
    "  adj_matrix <- amat(cpdag)\n",
    "  ig <- graph_from_adjacency_matrix(adj_matrix, mode = \"directed\")\n",
    "  if (igraph::is_dag(ig)) {\n",
    "    return(cpdag)\n",
    "  }\n",
    "  directed_arcs <- directed.arcs(cpdag)\n",
    "  undirected_arcs <- undirected.arcs(cpdag)\n",
    "  while (nrow(undirected_arcs) > 0) {\n",
    "    arc <- undirected_arcs[1, , drop = FALSE]\n",
    "    cpdag <- set.arc(cpdag, from = arc[1, 1], to = arc[1, 2])\n",
    "    undirected_arcs <- undirected.arcs(cpdag)\n",
    "  }\n",
    "  return(cpdag)\n",
    "}\n",
    "\n",
    "# Define train_bn function\n",
    "train_bn <- function(data, algorithm, score = NULL) {\n",
    "  if (any(is.na(data))) {\n",
    "    stop(\"The data contains missing values.\")\n",
    "  }\n",
    "  \n",
    "  if (algorithm %in% c(\"hc\", \"tabu\") && !is.null(score)) {\n",
    "    bn <- bnlearn::hc(data, score = score)\n",
    "  } else if (algorithm == \"tabu\" && !is.null(score)) {\n",
    "    bn <- bnlearn::tabu(data, score = score)\n",
    "  } else if (algorithm == \"gs\") {\n",
    "    bn <- bnlearn::gs(data)\n",
    "    bn <- bnlearn::cpdag(bn)\n",
    "    bn <- cpdag_to_dag(bn)\n",
    "  } else if (algorithm == \"iamb\") {\n",
    "    bn <- bnlearn::iamb(data)\n",
    "    bn <- bnlearn::cpdag(bn)\n",
    "    bn <- cpdag_to_dag(bn)\n",
    "  } else {\n",
    "    stop(\"Unsupported algorithm or missing score for algorithm.\")\n",
    "  }\n",
    "  \n",
    "  bn.fit(bn, data)\n",
    "}\n",
    "\n",
    "# Define a function to evaluate the Bayesian network model\n",
    "evaluate_bn <- function(testData, bn_fitted, target_var) {\n",
    "  predictions <- predict(bn_fitted, data = testData, node = target_var)\n",
    "  mean(predictions == testData[[target_var]])\n",
    "}\n",
    "\n",
    "custom_model <- list(\n",
    "  type = c(\"Classification\", \"Regression\"),\n",
    "  library = \"bnlearn\",\n",
    "  loop = NULL,\n",
    "  parameters = data.frame(parameter = c(\"algorithm\", \"score\"),\n",
    "                          class = c(\"character\", \"character\"),\n",
    "                          label = c(\"Algorithm\", \"Score\")),\n",
    "  grid = function(x, y, len = NULL, search = \"grid\") {\n",
    "    algorithms <- c(\"hc\", \"tabu\", \"gs\", \"iamb\")\n",
    "    scores <- c(\"aic\", \"bic\")\n",
    "    expand.grid(algorithm = algorithms, score = scores)\n",
    "  },\n",
    "  fit = function(x, y, wts, param, lev, last, classProbs, ...) {\n",
    "    data <- as.data.frame(x)\n",
    "    data$income <- y\n",
    "    \n",
    "    print(\"Fitting model with parameters:\")\n",
    "    print(param)\n",
    "    \n",
    "    if (any(is.na(data))) {\n",
    "      stop(\"The data contains missing values.\")\n",
    "    }\n",
    "    \n",
    "    # Additional debug info\n",
    "    if (!param$score %in% c(\"aic\", \"bic\")) {\n",
    "      stop(\"Invalid score parameter: \", param$score)\n",
    "    }\n",
    "    \n",
    "    train_bn(data, param$algorithm, param$score)\n",
    "  },\n",
    "  predict = function(modelFit, newdata, submodels = NULL) {\n",
    "    if (any(is.na(newdata))) {\n",
    "      stop(\"The new data contains missing values.\")\n",
    "    }\n",
    "    predict(modelFit, newdata)\n",
    "  },\n",
    "  prob = function(modelFit, newdata, submodels = NULL) {\n",
    "    if (any(is.na(newdata))) {\n",
    "      stop(\"The new data contains missing values.\")\n",
    "    }\n",
    "    predict(modelFit, newdata, type = \"prob\")\n",
    "  },\n",
    "  predictors = function(x, ...) {\n",
    "    names(x$bn)\n",
    "  },\n",
    "  varImp = NULL,\n",
    "  levels = function(x) x$lev,\n",
    "  tags = c(\"Bayesian Network\", \"Graphical Models\"),\n",
    "  sort = function(x) x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "bn_pred <- function(data, outer_folds, inner_folds){############adjust##############\n",
    "\n",
    "    # discretize the data\n",
    "    data <- discretize_df(data)\n",
    "\n",
    "    if (any(is.na(data))) {\n",
    "        stop(\"Data contains NA values after discretization\")\n",
    "    }\n",
    "\n",
    "    # adjust evaluation metric to fit both numeric and factored targets\n",
    "    summaryFunctionType <- if (is.numeric(data$income)) defaultSummary else multiClassSummary\n",
    "    # metric: train() uses per default RSME and Accuracy for numeric and factored targets\n",
    "\n",
    "    #  set control args\n",
    "    outer_control <- caret::trainControl(method = \"cv\", number = outer_folds,\n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "        \n",
    "    inner_control <- caret::trainControl(method = \"cv\", number = inner_folds, \n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "\n",
    "    # Define the grid for hyperparameter tuning\n",
    "    algorithms <- c(\"hc\", \"tabu\", \"gs\", \"iamb\") ############adjust##############\n",
    "    scores <- c(\"aic\", \"bic\") ############adjust##############\n",
    "\n",
    "    # Create grid\n",
    "    tunegrid <- expand.grid(algorithm = algorithms, score = scores) ############adjust##############\n",
    "\n",
    "    # Initialize variables to store results\n",
    "    outer_results <- list()\n",
    "\n",
    "    outer_cv_folds = createFolds(data$income, k = outer_folds)\n",
    "    \n",
    "    # Outer loop: Cross-validation for model evaluation\n",
    "    for (i in seq_along(outer_folds)) {\n",
    "        \n",
    "        # Split data into outer folds\n",
    "        outer_test_index = outer_cv_folds[[i]]\n",
    "        outer_testData = data[outer_test_index,]\n",
    "        outer_trainData  = data[-outer_test_index,]\n",
    "        print(\"outer data folds\")\n",
    "        print(any(is.na(outer_trainData)))\n",
    "        print(any(is.na(outer_testData)))\n",
    "        if (any(is.na(outer_trainData))) {\n",
    "        print(colSums(is.na(outer_trainData)))}\n",
    "\n",
    "        print(\"before train\")\n",
    "        # Hyperparameter tuning using inner CV\n",
    "        # No need for inner loop because \"train\" does k-fold CV already\n",
    "        model <- caret::train(income ~ ., \n",
    "                        data = outer_trainData, \n",
    "                        method = custom_model, ############adjust##############\n",
    "                        tuneGrid = tunegrid, \n",
    "                        trControl = inner_control)#,\n",
    "                        #metric = metricType)\n",
    "            \n",
    "\n",
    "        # Store the best hyperparameters\n",
    "        best_hyperparameters <- model$bestTune\n",
    "\n",
    "        # Train the final model on the outer training set with the best hyperparameters\n",
    "        final_model <- caret::train(income ~ ., \n",
    "                             data = outer_trainData, \n",
    "                             method = \"rpart\",############adjust##############\n",
    "                             trControl = outer_control, \n",
    "                             tuneGrid = best_hyperparameters)\n",
    "\n",
    "        # Testing the final model on the outer test set\n",
    "        predictions <- predict(final_model, newdata = outer_testData)\n",
    "        \n",
    "        if (is.numeric(data$income)) {\n",
    "            eval <- postResample(predictions, outer_testData$income) # postResample is a useful caret function\n",
    "        } else if (is.factor(data$income)) {\n",
    "            eval <- confusionMatrix(predictions, outer_testData$income)\n",
    "        } else {\n",
    "            stop(\"The predicted target has to be numeric or factor.\")\n",
    "        }\n",
    "\n",
    "        # Store the evaluation metrics for this outer fold\n",
    "        outer_results[[i]] <- eval\n",
    "    }\n",
    "\n",
    "    # Average the evaluation metrics over the outer folds\n",
    "    eval_avg_outer_fold <- mean(unlist(outer_results)) # Calculate the mean performance over all outer folds\n",
    "\n",
    "    # Return the average evaluation metrics\n",
    "    return(eval_avg_outer_fold)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "mlp_pred <- function(data, outer_folds, size_steps, decay_steps, inner_folds){\n",
    "    # adjust evaluation metric to fit both numeric and factored targets\n",
    "    summaryFunctionType <- if (is.numeric(data$income)) defaultSummary else multiClassSummary\n",
    "    # metric: train() uses per default RSME and Accuracy for numeric and factored targets\n",
    "\n",
    "    #  set control args\n",
    "    outer_control <- caret::trainControl(method = \"cv\", number = outer_folds,\n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "        \n",
    "    inner_control <- caret::trainControl(method = \"cv\", number = inner_folds, \n",
    "                                  summaryFunction = summaryFunctionType,\n",
    "                                  verboseIter = FALSE,\n",
    "                                  allowParallel = TRUE)\n",
    "\n",
    "    # Define the grid for hyperparameter tuning\n",
    "    size_values <- seq(1, 10, length.out = size_steps)\n",
    "    decay_values <- 10^seq(log10(0.0001), log10(0.01), length.out = decay_steps)\n",
    "\n",
    "    # Create grid\n",
    "    tunegrid <- expand.grid(size = size_values, decay = decay_values)\n",
    "\n",
    "    # Initialize variables to store results\n",
    "    outer_results <- list()\n",
    "\n",
    "    outer_cv_folds = createFolds(data$income, k = outer_folds)\n",
    "    \n",
    "    # Outer loop: Cross-validation for model evaluation\n",
    "    for (i in seq_along(outer_folds)) {\n",
    "        \n",
    "        # Split data into outer folds\n",
    "        outer_test_index = outer_cv_folds[[i]]\n",
    "        outer_testData = data[outer_test_index,]\n",
    "        outer_trainData  = data[-outer_test_index,]\n",
    "        \n",
    "        # Hyperparameter tuning using inner CV\n",
    "        # No need for inner loop because \"train\" does k-fold CV already\n",
    "        mlp_model <- caret::train(income ~ ., \n",
    "                           data = outer_trainData, \n",
    "                           method = \"nnet\", \n",
    "                           tuneGrid = tunegrid, \n",
    "                           trControl = inner_control)#,\n",
    "                           #metric = metricType)\n",
    "            \n",
    "\n",
    "        # Store the best hyperparameters\n",
    "        best_hyperparameters <- mlp_model$bestTune\n",
    "        print(\"best HP\")\n",
    "        print(mlp_model$bestTune)\n",
    "\n",
    "        # Train the final model on the outer training set with the best hyperparameters\n",
    "        final_model <- caret::train(income ~ ., \n",
    "                             data = outer_trainData, \n",
    "                             method = \"nnet\", \n",
    "                             trControl = outer_control, \n",
    "                             tuneGrid = best_hyperparameters)\n",
    "\n",
    "        # Testing the final model on the outer test set\n",
    "        predictions <- predict(final_model, newdata = outer_testData)\n",
    "        \n",
    "        if (is.numeric(data$income)) {\n",
    "            eval <- postResample(predictions, outer_testData$income) # postResample is a useful caret function\n",
    "        } else if (is.factor(data$income)) {\n",
    "            eval <- confusionMatrix(predictions, outer_testData$income)\n",
    "        } else {\n",
    "            stop(\"The predicted target has to be numeric or factor.\")\n",
    "        }\n",
    "\n",
    "        # Store the evaluation metrics for this outer fold\n",
    "        outer_results[[i]] <- eval\n",
    "    }\n",
    "\n",
    "    # Average the evaluation metrics over the outer folds\n",
    "    eval_avg_outer_fold <- mean(unlist(outer_results)) # Calculate the mean performance over all outer folds\n",
    "\n",
    "    # Return the average evaluation metrics\n",
    "    return(eval_avg_outer_fold)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "simulation <- function(data, nrun = 10, outer_folds = 5, inner_folds = 5, cp_steps = 10, #CART params\n",
    "                                                                mtry_steps = 10, ntree_steps= 10, # RF params\n",
    "                                                                nrounds_steps = 10, max_depth_steps = 10, eta_steps = 10, gamma_steps = 10, colsample_bytree_steps = 10, min_child_weight_steps = 10, subsample_steps = 10, # XGB params\n",
    "                                                                size_steps = 10, decay_steps = 10 # MLP params\n",
    "                                                                ){\n",
    "    \n",
    "    # create empty list to store evaluation dataframes\n",
    "    eval_list <- list()\n",
    "\n",
    "    # set inital seed\n",
    "    s <- 1234\n",
    "    for (i in 1:nrun){\n",
    "        # vary seed with each run\n",
    "        s <- s + 1\n",
    "\n",
    "        # create synthetic data\n",
    "        # data <- gen_data()\n",
    "\n",
    "        # prediction model with nested CV and grid search\n",
    "        CART_eval <- cart_pred(data, outer_folds, cp_steps, inner_folds)\n",
    "        RF_eval <- rf_pred(data, outer_folds, mtry_steps, ntree_steps, inner_folds)\n",
    "        XGB_eval <- xgb_pred(data, outer_folds, inner_folds, nrounds_steps, max_depth_steps = 10, eta_steps = 10, gamma_steps = 10, colsample_bytree_steps = 10, min_child_weight_steps = 10, subsample_steps = 10)\n",
    "        BN_eval <- bn_pred(data, outer_folds, inner_folds)\n",
    "        MLP_eval <- mlp_pred(data, outer_folds, size_steps, decay_steps, inner_folds)\n",
    "\n",
    "        # bind results \n",
    "        eval <- rbind(CART_eval = CART_eval, RF_eval = RF_eval, XGB_eval = Boost_eval, BN_eval = BN_eval, MLP_eval = MLP_eval)\n",
    "\n",
    "        # ich glaube wenn es so verschachtelt ist und ich eine Liste und noch eine Liste habe, müsste ich es anders machen\n",
    "        # am besten wäre ein Dataframe und für jede Model-Art eine Zeile\n",
    "        eval_list[[i]] <- eval\n",
    "        print(c(\"run\", i, \"completed\"))\n",
    "        }\n",
    "\n",
    "    # average over all runs\n",
    "    sum_df <- Reduce(function(x, y) Map(`+`, x, y), eval_list)\n",
    "    eval_avg <- lapply(sum_df, function(col) col / length(eval_list))\n",
    "\n",
    "    # Convert the list back to a dataframe\n",
    "    # Store row names\n",
    "    rownames <- row.names(eval_list[[1]])\n",
    "\n",
    "    # Convert the list back to a dataframe\n",
    "    eval_avg <- as.data.frame(eval_avg)\n",
    "\n",
    "    # Set back the row names\n",
    "    row.names(eval_avg) <- rownames\n",
    "    \n",
    "    # returns\n",
    "    results <- list(eval_avg = eval_avg)\n",
    "    return(results)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "adult_res <- simulation(data = adult, nrun = 1, outer_folds = 2, inner_folds = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [],
   "source": [
    "cps_res <- simulation(data = cpspop, nrun = 1, outer_folds = 2, inner_folds = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "vscode": {
     "languageId": "r"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in gzfile(file, mode):\n",
      "\"kann komprimierte Datei '/Users/emmafoessing/Documents/Master/MA/Code/Master-Thesis/simulation/cps_CART_res.RData' nicht \"offnen. Grund evtl. 'No such file or directory'\"\n"
     ]
    },
    {
     "ename": "ERROR",
     "evalue": "Error in gzfile(file, mode): kann Verbindung nicht \"offnen\n",
     "output_type": "error",
     "traceback": [
      "Error in gzfile(file, mode): kann Verbindung nicht \"offnen\nTraceback:\n",
      "1. saveRDS(cps_res, file = (paste0(here(), \"/simulation/cps_CART_res.RData\")))",
      "2. gzfile(file, mode)"
     ]
    }
   ],
   "source": [
    "#Saving the data:\n",
    "saveRDS(cps_res, file = (paste0(here(), \"/simulation/cps_orig_res.RData\")))\n",
    "saveRDS(adult_res, file = (paste0(here(), \"/simulation/adult_orig_res.RData\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
